{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360, 640) (360, 640, 3)\n",
      "[208 246 231 269] [208 246 218 256] 0.8109640831758034\n",
      "scale_change 0.8109640831758034\n",
      "hypotheses\n",
      "<class 'list'>\n",
      "(4,)\n",
      "best_hypothesis (4,)\n",
      "(360, 640) (360, 640, 3)\n",
      "[203 241 226 264] [207 263 212 263] 1.0\n",
      "scale_change 1.0\n",
      "hypotheses\n",
      "<class 'list'>\n",
      "(4,)\n",
      "best_hypothesis (4,)\n",
      "(360, 640) (360, 640, 3)\n",
      "[193 241 216 264] [193 241 216 264] 0.0\n",
      "scale_change 0.0\n",
      "hypotheses\n",
      "<class 'list'>\n",
      "(4,)\n",
      "best_hypothesis (4,)\n",
      "(360, 640) (360, 640, 3)\n",
      "[183 241 206 264] [183 241 206 264] 0.0\n",
      "scale_change 0.0\n",
      "hypotheses\n",
      "<class 'list'>\n",
      "(4,)\n",
      "best_hypothesis (4,)\n",
      "(360, 640) (360, 640, 3)\n",
      "[178 236 201 259] [178 236 201 259] 0.0\n",
      "scale_change 0.0\n",
      "hypotheses\n",
      "<class 'list'>\n",
      "(4,)\n",
      "best_hypothesis (4,)\n",
      "(360, 640) (360, 640, 3)\n",
      "[178 236 201 259] [178 236 201 259] 0.0\n",
      "scale_change 0.0\n",
      "hypotheses\n",
      "<class 'list'>\n",
      "(4,)\n",
      "best_hypothesis (4,)\n",
      "(360, 640) (360, 640, 3)\n",
      "[178 236 201 259] [178 236 201 259] 0.0\n",
      "scale_change 0.0\n",
      "hypotheses\n",
      "<class 'list'>\n",
      "(4,)\n",
      "best_hypothesis (4,)\n",
      "(360, 640) (360, 640, 3)\n",
      "[173 231 196 254] [173 231 196 254] 0.0\n",
      "scale_change 0.0\n",
      "hypotheses\n",
      "<class 'list'>\n",
      "(4,)\n",
      "best_hypothesis (4,)\n",
      "(360, 640) (360, 640, 3)\n",
      "[168 226 191 249] [168 226 191 249] 0.0\n",
      "scale_change 0.0\n",
      "hypotheses\n",
      "<class 'list'>\n",
      "(4,)\n",
      "best_hypothesis (4,)\n",
      "(360, 640) (360, 640, 3)\n",
      "[178 226 201 249] [183 226 183 226] 1.0\n",
      "scale_change 1.0\n",
      "hypotheses\n",
      "<class 'list'>\n",
      "(4,)\n",
      "best_hypothesis (4,)\n",
      "(360, 640) (360, 640, 3)\n",
      "[173 221 196 244] [173 221 196 244] 0.0\n",
      "scale_change 0.0\n",
      "hypotheses\n",
      "<class 'list'>\n",
      "(4,)\n",
      "best_hypothesis (4,)\n",
      "(360, 640) (360, 640, 3)\n",
      "[183 221 206 244] [183 221 206 244] 0.0\n",
      "scale_change 0.0\n",
      "hypotheses\n",
      "<class 'list'>\n",
      "(4,)\n",
      "best_hypothesis (4,)\n",
      "(360, 640) (360, 640, 3)\n",
      "[178 216 201 239] [178 216 201 239] 0.0\n",
      "scale_change 0.0\n",
      "hypotheses\n",
      "<class 'list'>\n",
      "(4,)\n",
      "best_hypothesis (4,)\n",
      "(360, 640) (360, 640, 3)\n",
      "[183 221 206 244] [183 221 206 244] 0.0\n",
      "scale_change 0.0\n",
      "hypotheses\n",
      "<class 'list'>\n",
      "(4,)\n",
      "best_hypothesis (4,)\n",
      "(360, 640) (360, 640, 3)\n",
      "[183 221 187 225] [183 222 183 222] 1.0\n",
      "scale_change 1.0\n",
      "hypotheses\n",
      "<class 'list'>\n",
      "(4,)\n",
      "best_hypothesis (4,)\n",
      "(360, 640) (360, 640, 3)\n",
      "[183 221 187 225] [183 221 187 225] 0.0\n",
      "scale_change 0.0\n",
      "hypotheses\n",
      "<class 'list'>\n",
      "(4,)\n",
      "best_hypothesis (4,)\n",
      "(360, 640) (360, 640, 3)\n",
      "[188 226 192 230] [188 226 192 230] 0.0\n",
      "scale_change 0.0\n",
      "hypotheses\n",
      "<class 'list'>\n",
      "(4,)\n",
      "best_hypothesis (4,)\n",
      "(360, 640) (360, 640, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y9/2rxtmq5d4rs75j3fynzjqhxh0000gn/T/ipykernel_88678/3320627982.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  c_o = np.cumsum(h_o) / np.sum(h_o)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "attempt to get argmin of an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 193\u001b[0m\n\u001b[1;32m    189\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 193\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[2], line 174\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m frames \u001b[38;5;241m=\u001b[39m (cv2\u001b[38;5;241m.\u001b[39mimread(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(sequence_folder, f)) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m frame_files)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# for frame in frames:\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m#     cv2.imshow('Object Tracking', frame)\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m#     cv2.waitKey(0)\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame, object_hypothesis, likelihood_map \u001b[38;5;129;01min\u001b[39;00m track_object(frames, initial_hypothesis):\n\u001b[1;32m    175\u001b[0m     vis_frame \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    176\u001b[0m     top, left, bottom, right \u001b[38;5;241m=\u001b[39m object_hypothesis\n",
      "Cell \u001b[0;32mIn[2], line 141\u001b[0m, in \u001b[0;36mtrack_object\u001b[0;34m(frames, initial_hypothesis, lambda_s, max_scale_change)\u001b[0m\n\u001b[1;32m    137\u001b[0m surrounding_region \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, top\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m20\u001b[39m), \u001b[38;5;28mmin\u001b[39m(frame\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], bottom\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m20\u001b[39m)),\n\u001b[1;32m    138\u001b[0m                       \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, left\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m20\u001b[39m), \u001b[38;5;28mmin\u001b[39m(frame\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], right\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m20\u001b[39m)))\n\u001b[1;32m    140\u001b[0m c_o, c_s \u001b[38;5;241m=\u001b[39m compute_cumulative_histograms(likelihood_map, object_region, surrounding_region)\n\u001b[0;32m--> 141\u001b[0m tau_star \u001b[38;5;241m=\u001b[39m adaptive_threshold(c_o, c_s)\n\u001b[1;32m    143\u001b[0m scale_estimate \u001b[38;5;241m=\u001b[39m estimate_scale(likelihood_map, object_hypothesis, tau_star)\n\u001b[1;32m    145\u001b[0m prev_area \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mprod(object_hypothesis[\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;241m-\u001b[39m object_hypothesis[:\u001b[38;5;241m2\u001b[39m])\n",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m, in \u001b[0;36madaptive_threshold\u001b[0;34m(c_o, c_s)\u001b[0m\n\u001b[1;32m     22\u001b[0m valid_thresholds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(c_o \u001b[38;5;241m+\u001b[39m c_s \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     23\u001b[0m costs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m c_o[valid_thresholds] \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(c_o[valid_thresholds[\u001b[38;5;241m1\u001b[39m:]], [\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m c_s[valid_thresholds]\n\u001b[0;32m---> 24\u001b[0m tau_star \u001b[38;5;241m=\u001b[39m valid_thresholds[np\u001b[38;5;241m.\u001b[39margmin(costs)] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tau_star\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36margmin\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:1338\u001b[0m, in \u001b[0;36margmin\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;124;03mReturns the indices of the minimum values along an axis.\u001b[39;00m\n\u001b[1;32m   1253\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m-> 1338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margmin\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[0;31mValueError\u001b[0m: attempt to get argmin of an empty sequence"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import ndimage\n",
    "\n",
    "def compute_likelihood_map(frame, object_hypothesis, object_model):\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    object_hist = object_model['histogram']\n",
    "    likelihood_map = cv2.calcBackProject([hsv], [0, 1], object_hist, [0, 180, 0, 256], 1)\n",
    "    likelihood_map = cv2.normalize(likelihood_map, None, 0, 1, cv2.NORM_MINMAX)\n",
    "    return likelihood_map\n",
    "\n",
    "def compute_cumulative_histograms(likelihood_map, object_region, surrounding_region):\n",
    "    h_o = np.histogram(likelihood_map[tuple(object_region)], bins=256, range=(0, 1))[0]\n",
    "    h_s = np.histogram(likelihood_map[tuple(surrounding_region)], bins=256, range=(0, 1))[0]\n",
    "    c_o = np.cumsum(h_o) / np.sum(h_o)\n",
    "    c_s = np.cumsum(h_s) / np.sum(h_s)\n",
    "    return c_o, c_s\n",
    "\n",
    "def adaptive_threshold(c_o, c_s):\n",
    "    \n",
    "    valid_thresholds = np.where(c_o + c_s >= 1)[0]\n",
    "    costs = 2 * c_o[valid_thresholds] - np.append(c_o[valid_thresholds[1:]], [1]) + c_s[valid_thresholds]\n",
    "    tau_star = valid_thresholds[np.argmin(costs)] / 255.0\n",
    "    return tau_star\n",
    "\n",
    "def estimate_scale(likelihood_map, object_hypothesis, tau):\n",
    "    segmentation = likelihood_map > tau\n",
    "    safe_region = np.zeros_like(segmentation)\n",
    "    top, left, bottom, right = object_hypothesis\n",
    "    safe_region[top:bottom, left:right] = True\n",
    "    safe_region[top+1:bottom-1, left+1:right-1] = False  # Inner 80% (approximated)\n",
    "    \n",
    "    labeled, num_features = ndimage.label(segmentation)\n",
    "    object_mask = np.zeros_like(segmentation)\n",
    "    \n",
    "    for label in range(1, num_features + 1):\n",
    "        component = labeled == label\n",
    "        if np.any(component & safe_region):\n",
    "            avg_likelihood = np.mean(likelihood_map[component])\n",
    "            if avg_likelihood > tau:\n",
    "                object_mask |= component\n",
    "    \n",
    "    if np.sum(object_mask) == 0:\n",
    "        return object_hypothesis\n",
    "    \n",
    "    rows, cols = np.where(object_mask)\n",
    "    top, bottom = np.min(rows), np.max(rows)\n",
    "    left, right = np.min(cols), np.max(cols)\n",
    "    return np.array([top, left, bottom, right])\n",
    "\n",
    "def generate_hypotheses(initial_hypothesis, scale_factors, translations, image_shape):\n",
    " \n",
    "    hypotheses = []\n",
    "    top, left, bottom, right = initial_hypothesis\n",
    "    height = bottom - top\n",
    "    width = right - left\n",
    "    \n",
    "    for scale in scale_factors:\n",
    "        # Scale the bounding box\n",
    "        new_height = int(height * scale)\n",
    "        new_width = int(width * scale)\n",
    "        \n",
    "        for dy, dx in translations:\n",
    "            # Translate the bounding box\n",
    "            new_top = max(0, top + dy)\n",
    "            new_left = max(0, left + dx)\n",
    "            new_bottom = min(image_shape[0], new_top + new_height)\n",
    "            new_right = min(image_shape[1], new_left + new_width)\n",
    "            \n",
    "            # Add the new hypothesis if it's within the image bounds\n",
    "            hypotheses.append((new_top, new_left, new_bottom, new_right))\n",
    "    \n",
    "    return hypotheses\n",
    "def compute_visual_score(likelihood_map, object_hypothesis):\n",
    "    top, left, bottom, right = object_hypothesis\n",
    "    return np.sum(likelihood_map[top:bottom, left:right])\n",
    "def compute_spatial_score(object_hypothesis, previous_center, sigma):\n",
    "\n",
    "    top, left, bottom, right = object_hypothesis\n",
    "    rows, cols = np.mgrid[top:bottom, left:right]\n",
    "    distances = (rows - previous_center[0])**2 + (cols - previous_center[1])**2\n",
    "    spatial_score = np.sum(np.exp(-distances / (2 * sigma**2)))\n",
    "    return spatial_score\n",
    "def find_best_hypothesis(likelihood_map, hypotheses, previous_center, sigma=10):\n",
    "    best_hypothesis = None\n",
    "    best_score = -np.inf\n",
    "\n",
    "    for hypothesis in hypotheses:\n",
    "        visual_score = compute_visual_score(likelihood_map, hypothesis)\n",
    "        spatial_score = compute_spatial_score(hypothesis, previous_center, sigma)\n",
    "        total_score = visual_score * spatial_score\n",
    "\n",
    "        if total_score > best_score:\n",
    "            best_score = total_score\n",
    "            best_hypothesis = hypothesis\n",
    "\n",
    "    return best_hypothesis\n",
    "def update_object_hypothesis(prev_hypothesis, scale_estimate, lambda_s,likelihood_map):\n",
    "    # Example usage\n",
    "    # initial_hypothesis = (100, 150, 200, 250)  # Example bounding box: (top, left, bottom, right)\n",
    "    scale_factors = [0.2, 1.0, .2]  # Scale down to 80%, keep original, scale up to 120%\n",
    "    translations = [(0, 0), (-5, -5), (5, 5), (-10, 0), (10, 0)]  # Various translations\n",
    "\n",
    "    # Assuming image shape is (height, width) \n",
    "    image_shape = (360, 640)  # Example image size\n",
    "    hypotheses = generate_hypotheses(prev_hypothesis, scale_factors, translations, image_shape)\n",
    "    print(\"hypotheses\")\n",
    "    print(type( hypotheses))\n",
    "    print(scale_estimate.shape)\n",
    "    best_hypothesis = find_best_hypothesis(likelihood_map, hypotheses, prev_hypothesis, sigma=10)\n",
    "    print(\"best_hypothesis\",np.array( best_hypothesis).shape)\n",
    "    return np.array( best_hypothesis)\n",
    "\n",
    "def update_object_model(frame, object_hypothesis):\n",
    "    top, left, bottom, right = object_hypothesis\n",
    "    object_region = frame[top:bottom, left:right]\n",
    "    hsv = cv2.cvtColor(object_region, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist([hsv], [0, 1], None, [180, 256], [0, 180, 0, 256])\n",
    "    cv2.normalize(hist, hist, 0, 1, cv2.NORM_MINMAX)\n",
    "    return {'histogram': hist}\n",
    "\n",
    "def track_object(frames, initial_hypothesis, lambda_s=0.9, max_scale_change=0.9):\n",
    "    object_hypothesis = initial_hypothesis\n",
    "    object_model = None\n",
    "    \n",
    "    for frame in frames:\n",
    "        if object_model is None:\n",
    "            object_model = update_object_model(frame, object_hypothesis)\n",
    "        \n",
    "        likelihood_map = compute_likelihood_map(frame, object_hypothesis, object_model)\n",
    "        print(likelihood_map.shape,frame.shape)\n",
    "\n",
    "\n",
    "        top, left, bottom, right = object_hypothesis\n",
    "        object_region = (slice(top, bottom), slice(left, right))\n",
    "        surrounding_region = (slice(max(0, top-20), min(frame.shape[0], bottom+20)),\n",
    "                              slice(max(0, left-20), min(frame.shape[1], right+20)))\n",
    "        \n",
    "        c_o, c_s = compute_cumulative_histograms(likelihood_map, object_region, surrounding_region)\n",
    "        tau_star = adaptive_threshold(c_o, c_s)\n",
    "        \n",
    "        scale_estimate = estimate_scale(likelihood_map, object_hypothesis, tau_star)\n",
    "        \n",
    "        prev_area = np.prod(object_hypothesis[2:] - object_hypothesis[:2])\n",
    "        new_area = np.prod(scale_estimate[2:] - scale_estimate[:2])\n",
    "        \n",
    "        scale_change = np.abs(new_area / prev_area - 1)\n",
    "        print(object_hypothesis, scale_estimate,scale_change)\n",
    "        print(\"scale_change\",scale_change)\n",
    "        # object_hypothesis = scale_estimate\n",
    "        # if scale_change <= max_scale_change:\n",
    "        object_hypothesis = update_object_hypothesis(object_hypothesis, scale_estimate, lambda_s,likelihood_map)\n",
    "            # object_model = update_object_model(frame, object_hypothesis)\n",
    "            # object_hypothesis = scale_estimate\n",
    "        yield frame, object_hypothesis, likelihood_map\n",
    "\n",
    "def main():\n",
    "    sequence_folder = 'sequence/'\n",
    "    frame_files = sorted([f for f in os.listdir(sequence_folder) if f.endswith('.jpg') or f.endswith('.png')])\n",
    "    \n",
    "    # Given object location\n",
    "    initial_bbox = (246, 208, 23, 23)  # Converted to integer pixel values for bounding box\n",
    "    \n",
    "    initial_hypothesis = np.array([initial_bbox[1], initial_bbox[0], \n",
    "                                   initial_bbox[1] + initial_bbox[3], \n",
    "                                   initial_bbox[0] + initial_bbox[2]])\n",
    "    \n",
    "    frames = (cv2.imread(os.path.join(sequence_folder, f)) for f in frame_files)\n",
    "    # for frame in frames:\n",
    "    #     cv2.imshow('Object Tracking', frame)\n",
    "    #     cv2.waitKey(0)\n",
    "    \n",
    "    for frame, object_hypothesis, likelihood_map in track_object(frames, initial_hypothesis):\n",
    "        vis_frame = frame.copy()\n",
    "        top, left, bottom, right = object_hypothesis\n",
    "        cv2.rectangle(vis_frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "        \n",
    "        color_map = cv2.applyColorMap((likelihood_map * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "        vis_likelihood = cv2.addWeighted(frame, 0.1, color_map, 0.9, 0)\n",
    "        \n",
    "        vis = np.hstack((vis_frame, vis_likelihood))\n",
    "        cv2.imshow('Object Tracking', vis)\n",
    "        # cv2.waitKey(0)\n",
    "        key = cv2.waitKey(0) & 0xFF\n",
    "        if key == 27:  # ESC key\n",
    "            break\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
